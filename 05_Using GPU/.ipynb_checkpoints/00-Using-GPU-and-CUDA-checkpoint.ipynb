{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is CUDA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most people confuse CUDA for a language or maybe an API. It is not.\n",
    "\n",
    "Itâ€™s more than that. CUDA is a parallel computing platform and programming model that makes using a GPU for general purpose computing simple and elegant. The developer still programs in the familiar C, C++, Fortran, or an ever expanding list of supported languages, and incorporates extensions of these languages in the form of a few basic keywords.\n",
    "\n",
    "These keywords let the developer express massive amounts of parallelism and direct the compiler to the portion of the application that maps to the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do I install PyTorch for GPU?\n",
    "\n",
    "Refer to video, its dependent on whether you have an NVIDIA GPU card or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do I know if I have CUDA available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "# True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPU and CUDA\n",
    "\n",
    "We've provided 2 versions of our yml file, a GPU version and a CPU version. To use GPU, you need to either manually create a virtual environment, please watch the video related to this lecture, as not every computer can run GPU, you need CUDA and an NVIDIA GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get Id of default device\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0\n",
    "torch.cuda.get_device_name(0) # Get name device with ID '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the current GPU memory usage by \n",
    "# tensors in bytes for a given device\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\A1\\archive\\Jupyter\\envs\\mygpu\\Lib\\site-packages\\torch\\cuda\\memory.py:444: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the current GPU memory managed by the\n",
    "# caching allocator in bytes for a given device\n",
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using CUDA instead of CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU\n",
    "a = torch.FloatTensor([1.,2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU\n",
    "a = torch.FloatTensor([1., 2.]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Models to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features=4, h1=8, h2=9, out_features=3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features,h1)    # input layer\n",
    "        self.fc2 = nn.Linear(h1, h2)            # hidden layer\n",
    "        self.out = nn.Linear(h2, out_features)  # output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(32)\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the discussions here: discuss.pytorch.org/t/how-to-check-if-model-is-on-cuda\n",
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpumodel = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(gpumodel.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/iris.csv')\n",
    "X = df.drop('target',axis=1).values\n",
    "y = df['target'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Tensors to .cuda() tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train).cuda()\n",
    "X_test = torch.FloatTensor(X_test).cuda()\n",
    "y_train = torch.LongTensor(y_train).cuda()\n",
    "y_test = torch.LongTensor(y_test).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(X_train, batch_size=60, shuffle=True)\n",
    "testloader = DataLoader(X_test, batch_size=60, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1  loss: 0.08985399\n",
      "epoch: 11  loss: 0.08782312\n",
      "epoch: 21  loss: 0.08593906\n",
      "epoch: 31  loss: 0.08418491\n",
      "epoch: 41  loss: 0.08254945\n",
      "epoch: 51  loss: 0.08102170\n",
      "epoch: 61  loss: 0.07959123\n",
      "epoch: 71  loss: 0.07824884\n",
      "epoch: 81  loss: 0.07698692\n",
      "epoch: 91  loss: 0.07579874\n",
      "epoch: 101  loss: 0.07467870\n",
      "epoch: 111  loss: 0.07362224\n",
      "epoch: 121  loss: 0.07262372\n",
      "epoch: 131  loss: 0.07167861\n",
      "epoch: 141  loss: 0.07078333\n",
      "epoch: 151  loss: 0.06993618\n",
      "epoch: 161  loss: 0.06913206\n",
      "epoch: 171  loss: 0.06836826\n",
      "epoch: 181  loss: 0.06764128\n",
      "epoch: 191  loss: 0.06694854\n",
      "epoch: 201  loss: 0.06628775\n",
      "epoch: 211  loss: 0.06565682\n",
      "epoch: 221  loss: 0.06505379\n",
      "epoch: 231  loss: 0.06447706\n",
      "epoch: 241  loss: 0.06392489\n",
      "epoch: 251  loss: 0.06339587\n",
      "epoch: 261  loss: 0.06288861\n",
      "epoch: 271  loss: 0.06240211\n",
      "epoch: 281  loss: 0.06193503\n",
      "epoch: 291  loss: 0.06148631\n",
      "epoch: 301  loss: 0.06105479\n",
      "epoch: 311  loss: 0.06063952\n",
      "epoch: 321  loss: 0.06023962\n",
      "epoch: 331  loss: 0.05985429\n",
      "epoch: 341  loss: 0.05948278\n",
      "epoch: 351  loss: 0.05912432\n",
      "epoch: 361  loss: 0.05877824\n",
      "epoch: 371  loss: 0.05844383\n",
      "epoch: 381  loss: 0.05812053\n",
      "epoch: 391  loss: 0.05780780\n",
      "epoch: 401  loss: 0.05750512\n",
      "epoch: 411  loss: 0.05721199\n",
      "epoch: 421  loss: 0.05692786\n",
      "epoch: 431  loss: 0.05665237\n",
      "epoch: 441  loss: 0.05638510\n",
      "epoch: 451  loss: 0.05612562\n",
      "epoch: 461  loss: 0.05587357\n",
      "epoch: 471  loss: 0.05562859\n",
      "epoch: 481  loss: 0.05539040\n",
      "epoch: 491  loss: 0.05515865\n",
      "epoch: 501  loss: 0.05493302\n",
      "epoch: 511  loss: 0.05471330\n",
      "epoch: 521  loss: 0.05449913\n",
      "epoch: 531  loss: 0.05429038\n",
      "epoch: 541  loss: 0.05408671\n",
      "epoch: 551  loss: 0.05388796\n",
      "epoch: 561  loss: 0.05369386\n",
      "epoch: 571  loss: 0.05350428\n",
      "epoch: 581  loss: 0.05331898\n",
      "epoch: 591  loss: 0.05313779\n",
      "epoch: 601  loss: 0.05296052\n",
      "epoch: 611  loss: 0.05278707\n",
      "epoch: 621  loss: 0.05261721\n",
      "epoch: 631  loss: 0.05245085\n",
      "epoch: 641  loss: 0.05228783\n",
      "epoch: 651  loss: 0.05212804\n",
      "epoch: 661  loss: 0.05197132\n",
      "epoch: 671  loss: 0.05181759\n",
      "epoch: 681  loss: 0.05166672\n",
      "epoch: 691  loss: 0.05151859\n",
      "epoch: 701  loss: 0.05137311\n",
      "epoch: 711  loss: 0.05123017\n",
      "epoch: 721  loss: 0.05108968\n",
      "epoch: 731  loss: 0.05095161\n",
      "epoch: 741  loss: 0.05081578\n",
      "epoch: 751  loss: 0.05068219\n",
      "epoch: 761  loss: 0.05055071\n",
      "epoch: 771  loss: 0.05042135\n",
      "epoch: 781  loss: 0.05029399\n",
      "epoch: 791  loss: 0.05016858\n",
      "epoch: 801  loss: 0.05004504\n",
      "epoch: 811  loss: 0.04992327\n",
      "epoch: 821  loss: 0.04980331\n",
      "epoch: 831  loss: 0.04968499\n",
      "epoch: 841  loss: 0.04956830\n",
      "epoch: 851  loss: 0.04945326\n",
      "epoch: 861  loss: 0.04933971\n",
      "epoch: 871  loss: 0.04922768\n",
      "epoch: 881  loss: 0.04911708\n",
      "epoch: 891  loss: 0.04900791\n",
      "epoch: 901  loss: 0.04890013\n",
      "epoch: 911  loss: 0.04879367\n",
      "epoch: 921  loss: 0.04868851\n",
      "epoch: 931  loss: 0.04858460\n",
      "epoch: 941  loss: 0.04848195\n",
      "epoch: 951  loss: 0.04838051\n",
      "epoch: 961  loss: 0.04828024\n",
      "epoch: 971  loss: 0.04818111\n",
      "epoch: 981  loss: 0.04808311\n",
      "epoch: 991  loss: 0.04798624\n",
      "TOTAL TRAINING TIME: 5.477474689483643\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epochs = 1000\n",
    "losses = []\n",
    "start = time.time()\n",
    "for i in range(epochs):\n",
    "    i+=1\n",
    "    y_pred = gpumodel.forward(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # a neat trick to save screen space:\n",
    "    if i%10 == 1:\n",
    "        print(f'epoch: {i:2}  loss: {loss.item():10.8f}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'TOTAL TRAINING TIME: {time.time()-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mygpu)",
   "language": "python",
   "name": "mygpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
